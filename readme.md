Aqui est√° um exemplo de **README.md** para o seu projeto Jupyter que utiliza o **RAG (Retrieval-Augmented Generation)** com **LangChain** e **Ollama** localmente, lendo um PDF da pasta `files`. Este README fornece instru√ß√µes sobre como configurar e usar o projeto.

---

# RAG com LangChain e Ollama: Leitura de PDF Local

Este projeto demonstra como utilizar **RAG (Retrieval-Augmented Generation)** com **LangChain** e **Ollama** localmente para processar um documento PDF armazenado na pasta `files`. O c√≥digo √© executado em um **Jupyter Notebook** e permite a extra√ß√£o de informa√ß√µes relevantes do PDF e a gera√ß√£o de respostas utilizando a combina√ß√£o de recupera√ß√£o e gera√ß√£o de texto.

## üõ†Ô∏è Tecnologias Utilizadas

- **LangChain**: Framework para criar pipelines de processamento de linguagem natural (NLP) integrando LLMs e outras ferramentas.
- **Ollama**: Servi√ßo para executar modelos de linguagem, usado aqui para embeddings e gera√ß√£o de texto.
- **PDF**: O documento de entrada √© um arquivo PDF armazenado localmente na pasta `files`.
- **RAG (Retrieval-Augmented Generation)**: T√©cnica de NLP que usa recupera√ß√£o de documentos para gerar respostas mais precisas.

## üì¶ Depend√™ncias

Para rodar este projeto, voc√™ precisar√° instalar algumas depend√™ncias. As principais s√£o:

- `langchain`
- `langchain-ollama`
- `unstructured`
- `PyPDF2` ou `pdfplumber` (dependendo de como voc√™ for processar o PDF)

Se voc√™ j√° tem um ambiente Python configurado, pode instalar todas as depend√™ncias com o seguinte comando:

```bash
pip install -r requirements.txt
```

Ou adicione diretamente as depend√™ncias ao seu `requirements.txt`:

```txt
langchain
langchain-ollama
unstructured[all-docs]
PyPDF2
```

### Criando o arquivo `requirements.txt`

```bash
ipykernel
langchain
langchain-community
langchain-ollama
chromadb
unstructured
unstructured[all-docs]
```

## ‚öôÔ∏è Como Usar

### Passo 1: Prepara√ß√£o do Ambiente

1. **Instale as depend√™ncias** com o comando mencionado anteriormente (`pip install -r requirements.txt`).
2. **Baixe e prepare o modelo Ollama**:
   - Certifique-se de que o modelo **Ollama** esteja instalado e configurado corretamente em sua m√°quina local.
   - Voc√™ pode verificar se o Ollama est√° funcionando executando o comando `ollama --version` no terminal.

### Passo 2: Estrutura do Projeto

A estrutura b√°sica do seu projeto ser√°:

```
/rag-langchain-ollama
‚îú‚îÄ‚îÄ files
‚îÇ   ‚îî‚îÄ‚îÄ DouglasModestoResume.pdf           # O arquivo PDF que ser√° processado
‚îú‚îÄ‚îÄ langchain_rag.ipynb              # O Jupyter Notebook com o c√≥digo
‚îú‚îÄ‚îÄ requirements.txt            # Arquivo com as depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md                   # Este arquivo de README
```

### Passo 3: Executando o Jupyter Notebook

1. Abra o **Jupyter Notebook** em seu terminal ou ambiente de desenvolvimento preferido:

   ```bash
   jupyter notebook
   ```

2. No navegador, abra o arquivo `notebook.ipynb` localizado na raiz do projeto.

3. Execute as c√©lulas do notebook. As principais etapas no notebook s√£o:
   - Leitura do arquivo PDF local (`files/documento.pdf`).
   - Utiliza√ß√£o de **LangChain** para carregar o PDF e extrair texto.
   - Uso do modelo **Ollama** para gerar embeddings e realizar a recupera√ß√£o de documentos relevantes.
   - Gera√ß√£o de respostas com base no conte√∫do recuperado.

### Passo 4: Personalizar o C√≥digo

O c√≥digo no Jupyter Notebook pode ser facilmente modificado para:
- Utilizar outros modelos de linguagem de sua escolha.
- Alterar o arquivo PDF de entrada para outros documentos.
- Modificar a l√≥gica de recupera√ß√£o e gera√ß√£o, conforme as necessidades do seu projeto.

## üìÑ Exemplo de Uso

Aqui est√° um exemplo b√°sico do fluxo de trabalho no Jupyter Notebook:

```python
# Importando as bibliotecas necess√°rias
from langchain_ollama import OllamaEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# Carregando o PDF local
pdf_path = './files/documento.pdf'
loader = PyPDFLoader(pdf_path)
documents = loader.load()

# Inicializando embeddings com Ollama
embedding = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma.from_documents(documents, embedding)

# Configurando o modelo de gera√ß√£o com LangChain
qa_chain = RetrievalQA.from_chain_type(
    llm=embedding,
    retriever=vectorstore.as_retriever()
)

# Fazendo uma pergunta ao modelo
query = "Qual √© o resumo do documento?"
response = qa_chain.run(query)

# Exibindo a resposta gerada
print(response)
```

### Passo 5: Exemplo de Pergunta ao Modelo

- **Pergunta**: Qual √© o resumo do documento?
- **Resposta gerada**: O modelo ir√° gerar uma resposta baseada nas informa√ß√µes extra√≠das do PDF.

## üöß Problemas Comuns

1. **Erro ao Carregar o PDF**: Verifique se o caminho para o arquivo PDF est√° correto e se o PDF est√° acess√≠vel.
2. **Erro de Conex√£o com Ollama**: Certifique-se de que o modelo Ollama est√° corretamente instalado e funcionando em sua m√°quina local.

## üîß Como Contribuir

Se voc√™ deseja contribuir para este projeto, siga os seguintes passos:

1. Fa√ßa um **fork** deste reposit√≥rio.
2. Crie uma nova **branch** (`git checkout -b minha-nova-funcionalidade`).
3. Realize as mudan√ßas necess√°rias e fa√ßa o **commit** (`git commit -m 'Adicionando uma nova funcionalidade'`).
4. **Push** para sua branch (`git push origin minha-nova-funcionalidade`).
5. Abra um **pull request** para revis√£o.

## üìù Licen√ßa

Este projeto √© licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para mais detalhes.
